{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11322939,"sourceType":"datasetVersion","datasetId":7082139}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"DiVxksUCOAl-","execution":{"iopub.status.busy":"2025-04-08T18:40:52.302994Z","iopub.execute_input":"2025-04-08T18:40:52.303282Z","iopub.status.idle":"2025-04-08T18:40:52.634065Z","shell.execute_reply.started":"2025-04-08T18:40:52.303260Z","shell.execute_reply":"2025-04-08T18:40:52.633303Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/geometry3k/data-00000-of-00001.arrow\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nfrom datasets import Dataset\n\ndef load_geometry3k_dataset():\n    \"\"\"\n    Loads the Geometry3K dataset from the Arrow file in the current directory.\n\n    :return: A Hugging Face Dataset object.\n    \"\"\"\n    arrow_file_path = \"/kaggle/input/geometry3k/data-00000-of-00001.arrow\"\n\n    # Load the dataset from the Arrow file\n    dataset = Dataset.from_file(arrow_file_path)\n\n    return dataset\n\ndef get_train_val_test_splits(dataset):\n    \"\"\"\n    Splits the dataset into train, validation, and test sets.\n    - Train: first 2101 samples\n    - Validation: next 300 samples\n    - Test: remaining samples\n    \"\"\"\n    total_samples = dataset.num_rows\n    train_end = min(2101, total_samples)\n    val_end = min(train_end + 300, total_samples)\n\n    train_dataset = dataset.select(range(0, train_end))\n    val_dataset   = dataset.select(range(train_end, val_end))\n    test_dataset  = dataset.select(range(val_end, total_samples))\n\n    return {\n        \"train\": train_dataset,\n        \"validation\": val_dataset,\n        \"test\": test_dataset\n    }\n\n# Load and split the dataset\ndataset = load_geometry3k_dataset()\ndataset_splits = get_train_val_test_splits(dataset)\n\n# Show basic info\nprint(\"Train samples:\", len(dataset_splits[\"train\"]))\nprint(\"Validation samples:\", len(dataset_splits[\"validation\"]))\nprint(\"Test samples:\", len(dataset_splits[\"test\"]))\n\n# Display the first training sample\nprint(dataset_splits[\"train\"][0])\n","metadata":{"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"JCuX3xVQOAl_","outputId":"2a415960-fd4f-4bcb-af69-d15de6619e26","execution":{"iopub.status.busy":"2025-04-08T18:40:52.635572Z","iopub.execute_input":"2025-04-08T18:40:52.635886Z","iopub.status.idle":"2025-04-08T18:40:53.293615Z","shell.execute_reply.started":"2025-04-08T18:40:52.635866Z","shell.execute_reply":"2025-04-08T18:40:53.292621Z"}},"outputs":[{"name":"stdout","text":"Train samples: 2101\nValidation samples: 300\nTest samples: 501\n{'images': [<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=648x405 at 0x7B7137DA03A0>], 'problem': '<image>Find $x$.', 'answer': '2', 'id': 84, 'choices': ['1', '2', '3', '4'], 'ground_truth': 'B'}\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"dataset.shape","metadata":{"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"pSCWTlwhOAmA","outputId":"a924274c-2c10-45b9-dd5d-8de7a7ed3883","execution":{"iopub.status.busy":"2025-04-08T18:40:53.294937Z","iopub.execute_input":"2025-04-08T18:40:53.295410Z","iopub.status.idle":"2025-04-08T18:40:53.300601Z","shell.execute_reply.started":"2025-04-08T18:40:53.295377Z","shell.execute_reply":"2025-04-08T18:40:53.299909Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"(2902, 6)"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"dataset","metadata":{"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"YHGhOosdOAmB","outputId":"146804f3-08ee-4574-aee6-7e0e960c56f5","execution":{"iopub.status.busy":"2025-04-08T18:40:53.301490Z","iopub.execute_input":"2025-04-08T18:40:53.301834Z","iopub.status.idle":"2025-04-08T18:40:53.320107Z","shell.execute_reply.started":"2025-04-08T18:40:53.301803Z","shell.execute_reply":"2025-04-08T18:40:53.319423Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['images', 'problem', 'answer', 'id', 'choices', 'ground_truth'],\n    num_rows: 2902\n})"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"from collections import Counter\n\nanswers = [sample[\"ground_truth\"] for sample in dataset]\nanswer_counts = Counter(answers)\nprint(answer_counts)","metadata":{"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"BjL_Y91fOAmB","outputId":"fbadccca-1a4c-40c4-85a1-265721fffc99","execution":{"iopub.status.busy":"2025-04-08T18:40:53.320813Z","iopub.execute_input":"2025-04-08T18:40:53.321039Z","iopub.status.idle":"2025-04-08T18:41:05.140056Z","shell.execute_reply.started":"2025-04-08T18:40:53.321022Z","shell.execute_reply":"2025-04-08T18:41:05.139072Z"}},"outputs":[{"name":"stdout","text":"Counter({'B': 914, 'C': 826, 'A': 630, 'D': 532})\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from IPython.display import display, Markdown\nimport html\n\nfirst_row = dataset[11]\n\n# Display the image\ndisplay(first_row['images'][0])\n\n# Escape any HTML-like tags (e.g., <image>) so Markdown doesn't mess with them\nproblem_text = html.escape(first_row['problem'])\n\n# Format and show all info\ninfo = f\"\"\"\n### Problem ID: {first_row['id']}\n\n**Problem:**\n{problem_text}\n\n**Choices:**\n{', '.join(first_row['choices'])}\n\n**Answer (Index):** {first_row['answer']}\n\n**Ground Truth Label:** {first_row['ground_truth']}\n\"\"\"\n\ndisplay(Markdown(info))\n","metadata":{"trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":427},"id":"2jwUCAlzOAmB","outputId":"6585c27e-8372-448e-e2ff-d774be303b54","execution":{"iopub.status.busy":"2025-04-08T18:41:05.141030Z","iopub.execute_input":"2025-04-08T18:41:05.141305Z","iopub.status.idle":"2025-04-08T18:41:05.155056Z","shell.execute_reply.started":"2025-04-08T18:41:05.141285Z","shell.execute_reply":"2025-04-08T18:41:05.154196Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=254x235>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAP4AAADrCAYAAABJjSt5AAAzoElEQVR4Ae19CZwVxbX+NxmISNiGfZthFxgGZB0QRsEEZHkmKihGnyg80SyixriEiMYl6kOM8owYo+Af1GRciMQtCqJmiIAsw4Aw7Pu+DMMuizDM/359qZ6aO33v7Xunu29336rfb6b7dld3nzpVX9Wpc06dSikNJKikOKA4kFQc+EFSlVYVVnFAcUDjgAK+agiKA0nIAQX8JKx0VWTFAQV81QYUB5KQAwr4SVjpqsiKAwr4qg0oDiQhBxTwk7DSVZEVBxTwVRtQHEhCDijgJ2GlqyIrDijgqzagOJCEHFDAT8JKV0VWHKiiWOBmDpSg6NOHkPPrWSiJSGYq6nTMwY8HDMJ/jfwZrmhVEykR86ubyc4BNeK7ugUcw4b8FVFAzwKU4MjaeZj1yiO4feBo/GnxQaiVV66u2IQTp0b8hFdBBALOH8CmZfu0DNV/+hheHdMFF1XIXopzRzdjwTtT8dc5m1BSUoBXn/4AV79/OzpWVeN+BXapCxoHFPDd3BCO7sDqNScDFNZAp8suR5/ubcKI8D2QfXlPpN/1c4yfUwSszMfq/behY/Oqbi6doi2BHFCifgKZH/nTpTizYTm+OMtcbXF5VpMwoL/wlirp6DOw+4Uf+3HgkPbghd/qoDhQngMK+OX54aJf32Pv5nU4RoqqdkC79IpCfnliS3DquxMXLqWjWcMflr+tfikOSBxQwJeY4a7T49i2ZnOQpMz2aFk7NTJ5pUVY8826YJ5LuyOzoZrFRWZYct9VwHdr/ZfswZq83Rp1tfp2REbEmirF2Y1f4Z0vigP5ayP7xhy0iZjfrYVWdDnFAdU8nOJ0jN8p3bseS3bRet8Al3fOMNDmixcGtPr7FuIvT76Epcze8ue4a2grqIoV/FFHIw4oedCIKwm/dh7HNxWiQKMjDbW+24qCgr0VqTp1EFs3LsaHL72BhcUB1Kdm4+7nf4G+aVGmBRXfpK4kGQdSVJRdN9b4KaydOho/fXqJeeLqDcb9f3kMd/RuAtWbm2dbsuZUbcSNNV96MOCxtzk6ZWkd0TenF/pcMQiDBvVGuzrKbh+daSoHOaCA78Z2cGoHVi4sDlBWFW0nfIB/3ZEJJby7saK8S5PSAbmw7s7v3oiC4ySsCXq1a6hA78I68jpJCviuq8ESHN22HkGLfBt0bFHTdRQqgrzPAQV819Xhd9hYUADN4bZ5D3TOiOax57oCKII8wAEFfLdVUsADb1PBHo2qqpddguZqcu+2GvIFPQr4bqvGI1uwouBEgKoa6NqtNeq4jT5Fjy84oIDvsmos2bkBizU5vym6tWkQeUWey2hX5HiHAwr4rqqrs9i7fhU0D/3ULHRu/SNXUaeI8Q8HlOeef+pSlURxwDQH1IhvmlUqo+KAfziggO+fulQlURwwzQHlsmuaVd7PeOrMOews+s50QapfVAXNGyg9g2mGeSijAr6HKisaqYeOncbBY2dQuO0wTpw6h8Jdmt8vnis8FO3RqPd71aiCAS1roU71qujQrAYa1bkYDepUQ3qgY7g40EGo5C0OKOWet+pLp5aj9/qdR7Fl33Es2ngYeduOYemJc/p9J0+uaXQxslvU0jqErJZpuKR5bSc/r74VBwcU8ONgWiIeEUCfv6YIX6w7hA/3n4pKhhilmbFPuzQ9P8EZLZ08fU7rVJgvHulhbOtaGHppA6iOIBqnE3NfAT8xfDf1VYru+RuK8cWqAzGL6wT9Uz9ti6t6NjP1rVgyCV3Btn0nsO/wKW1KEWk6QVpu7dEI3VqnoXu7empqEAuzbcqrgG8TY+N9LUFVsLEYM+btwLQtxwxfQ9F6YIe6GpB+VK0Kur2ywjAfLz6d3Qj3XdvBEbDtCigO12w/gvzNh/HBmuKwUw/SNKxHE3RtWy8s3eqGvRxQwLeXv6bfvmHXUXweiKt397xgZN3QBwmW/p0aoGNGbdStVU2/fcfLSyt0EOwY9nx3VgceR9zXRmU5DjR2BEvWF+Gzb4sq0MgCkK5xOc1wde/m5cqkF06d2MYBBXzbWBv9xWJ0f+7TzYZzdgH2cOLxm3M34bbZ27UPMe/Wg6c0gD2YVRf3XH0JnnhvTTnAvTGkBW64oqUjo39o6cW0ZebiPeVoEvlI882XZzjeOYnvJ9tRAT8BNU7Af7Z0FybO3a6PyoIMKsVG98+IOhemhNB+cr72GEfOeQ/3w+QP1mHCkv3atdLnf6IdZ83fhhH/LIvfR2lgyphLE2qfZyeQt3If3ly4p0KHR/oeHNYG/bIaCZaoow0cUMC3ganhXhkO8EL5de1l6aYAyffc9OISHTTr7+upmdBkkBc/1k8Xn9lJPJS7Ws9P+t6/rg2G57QMR6pj11dsKkbu1zsqKC9VB2BvFSjg28tf7e2RAD9+UAsM7dU8JvH7mXcL9ZGd4vutg9pq3yGIhKJPdAaieKTh9cA22rIOgdLFs6M66x2EyJuII6WA3Lxt5egjHaoDsKc2FPDt4av+1gWF+3HfzHXlRHrRmMPN3fWHDU5kcBO4fx7bTe80qExLn7hIeyrciM7n73yrUKeH0oZdZj8D8qNeCtcBsKwPXnOJcg6KykFzGRTwzfEp5lwUr5/7cEM5RRZBxhE+XhGboBjy/GIdtDvH96kwNUi5/0uN1pf6N8O4n3UwpJuj/2O5q8qJ11QOOmX2MyQq5GK4DoB0/nJoW1dIKSEke+qnAr7F1UVQzfzPNl3bLl7PEThWkV48K44PTV+ugzXciC7ycIScelcv8ajh8fP83Xjk4016R8KOKRFmP0PiLlxkB/DXzzbpUxtedpuUEol+t95TwLewZkLFaL7aqhGKIB38djDoNk1fk8Z0M6R8ykfr9Hmy0OwbZrxwkdMDN5n9wtFqpKAUZku1gjAc18JfV8APzxvTd4wUZ5zHT7q5kyVzUnnuztFu9v29w4q6smbfaCoQrlDyc8xD+hNt9jOilXTKZlA1+htxKfo1FYgjOo8i5iAoaVqTteXUtL99b7YloOfHOSKLNPmGDmFBzzzyApyDR0+Lx6IeqXegJYCAZ+IiICoKCTQ3JdLJjo+jPRNXJFIS4hSHHbBK5jiggG+OT4a5KH4THGKlHEFD8NC8ZtUadXrnCZ99ThuiObbUr1W2AQeX7MaSuJyWHRYVgyLR+YduwZxruyXRZZlTnTk3ddDm+6SLi4T6P7MAnBKoFJ0DCvjReVQhB0cW2tLFnJsZCEorR3m+k41YuORSpKXWPVqS/fjX7T4RLXuF++ywaA1Y/quuOqjY8dCawI7OTYkrD2eN6wkqMpk4+tOb0W10uolnghYFfMEJk0eK9vdMW65rmQnI+WOy8PCNWZaN8iSFnQu97UT62x1dTb9fiMH03Y83ceUc3YDFu4RIzQ7PTSI1FXv0ZZClFHbIVHK6ic5468Gu5xTwY+AsQT98Sr4uelO054gTTfyO4RN6VvrdiykEdQaxRLXJah7caFNMEfSXxnjC0T9UpOZaAIrUtGC4JQkphR0wO2Im6lzYQSvwG9eSAr4xXypcpQce5/MivBVHQor2dpiSCCqx2IZiLFfUxZIapwUVdHzGijmvkUhN12DqH9wELHbA7IgF+NnxsZNih61SeQ4o4Jfnh+Evgj5neqF+j2IlR0KrFHj6iwMnVKLRpVakx0Zmxvydlo1riMdRdMQapRw7ODoE0XFIJOofaNFwE7BIJ6co8ryfUpqbaBT8S+RRAT8K9zlXlEFPTXI4V9gorzJ1e+L7a3WpgiCLR6Jg5FuR9h+Jf54v3iEfvWD2Y4fMeT8VrkyU0gh+N01PZJ4m4lwBPwLXZS84ZuMc0o4YdoIEaqNF7DpOJeL16WfDF+IuI/Banbxg9iMPqHAVSj+Cn9MTSm8qAQr4YVqBDHqhubdDiSc+T1FUmAf5vfEjOopbcR0ZA59pQ5G1I74ghsDygtmPNArwk3ZKbwr8CviiHZc7hoLeLs29/NFYvPPk58Kdi3DatAzYqYDzgtnPCPxWKD3D8d4L19WIH1JLHA2E+y1HXoI+nnl2yGsj/ozVOy/iyy7c5E43IsWybZZ4JpYjR3+3m/1CwX/L1BVJrfBTwJdaeKj2nn7xdoM+Hu88ieSwp9zeSiTGv3ciud3sJ4NfKPySVduvgH8BEQSgrL2nIs/OOT0/SxE8Xu+8aECWHX646YVTiR2lm81+oeAfN/1bW6dCTvE91u8o4Ac4xl6fop9IToCe36qMd56gNdJRrLQTm2dGymv1PTeb/Qh+YeqjDiQZPfySHvgcddnrC488aoDtHukJssp655kBKjeyZOKGmolIbjb7ccGTcPKhhx8DkSZTSnrgM/ac8InnKMDRwO5khXeeGRq5nTUTO7VELat1q9mPdNHJR/g7UKGbTKv6khr4DDIhHGbY+5tZ9moGcNHyWOGdF+0bvN+6cXCxDs8PHjvDQ8KSG81+BD+tNiLRjyJZzHxJC3xWsNhhhr1+PD7xosHEcrTKO8/MN+vXLtPsF26z3oPPDA1yHjea/aiMpE5HJOp67PR7EN9J9DEpgc+KlZV5TpjtWNFWe+dFazyyKXLPIWsW60T7ppn7bjP7UacjvPs4LaLS1e8pKYHPinVamceGZLV3npnGKRRYO4qdM+mZocttZr/bB7fVlX1cEu33+X7SAZ9OOvJad1a4E8kO7zwzdF+aHpznC12GmWeczOMWsx+nIZzuicT9BhKlEBU02HlMKuBTxOd2ViI5Na+3yztPlCPSsWndsnm+W73U3GL2oxQi4g1QIqQS1q8pqYAvi/jxrnWPtSGws7HLO88MLfGG2zbzbivzcMSlKTXRQT4pgYjpEaUkv4r8SQP8UIeZeNe6x9rY7fbOi0ZPZcJtR3u3HffdYPYLFfn9qOVPGuA/Pmu93k6566oTKbSzEdtZO/Ft8Y3KhtsW73HymGizX6jI70evvqQAPh11hHcezTbyAha7GrSRd55d34r2XhEiuzLhtqN9w477iTT7cYNTsdaBXn1+c+zxPfAppnGvNSY66jilxZe98xinT7ap2wGSSO+0Ktx2pG/YdS9RZj9KHY8Pb68Xa9rcLfq5H058D3yKacJmz73pWaF2p1DvPDvj9Jkpi9Xhts180+o8iTD7Ud8gpCUq+vwUrNPXwKe4LaLpUGxzQqHntHeeGYDZEW7bzHetzpMIs989V5fpg2Q9kdVlc/p9vgZ+bt42nZ8PDiuLB69ftOFE9s57bVRWxJ1tbfi84SvtDLdt+EEbLzpt9uNUQ7jzUk/kl0CdvgV+6GjvxBr7UO88iopuSASLWH5qR7jtRJTRSbPfzQNa6kV87tPN+rmXT3wLfKdHe9k7j9MKp5b4mm18dofbNkuHlfnYoTkR5JMmUb+N+r4EvtOjfah33qSbOzmiRIwFREKzL8yasTzr9rxOmP3kUf/DpXvczpKo9PkS+Hkr9+kFd2Jun2jvPL2wEU7aNCkLyuE3mzSLbbfZTx71qeH3Og99CXzZbm/33D50tV8ivPMi4F2/lYhw2/rHHTyx0+x3Vfcmekk+L9irn3vxxHfAJxBlu72dlcIpRehqPzu/V5l3y96KTobbrgzN8T5rl9mP7xULeGgmZv17NfkO+PL8a0CXxrbWi5u888wUVLigJiLcthn6rMxjl9lvdP8MnUx5Sqlf9MiJr4DPHlgEnGDEXHmBitX1IQfqpHdXor3zzJTvkgbBbbUSFW7bDI1W57Ha7MepozCNvrnQu0o++/1Xra7JCO/7ZPEu/e6wHmXzMf2iRSf0zpMDdVZ2Z1uLyIr6Gm0jzYBiilMhWiI4KiZDEma/gYFtyBlZh+VnFKYP1hSDTlax+lvc2qMRlgZEfVpIqOSTp1Gx8fM8Th/cjJVLFmNp/rdYtWE18uevxZHUDHQb0BUdsnqib99+6NujFWpXSYnt1VFyp5QGUpQ8nrl97aSFWmWwR17yRH9b6CZguPMKN2FgYuCIWBuOLYSZeCl9zblHPNP6+3pWosFqr/DkP3ba9K4U9cdCvDGkBW64oqXpjpDvSJ+4SCs/7fux78VQinOH12LOjJfw/JTZ2FESiZWpqNv/Xrww8U7kNLkoUsaY7vlG1GdlCBv1uJxmMTEhlswz/7NNbzScTngF9Cyj28Jtx8J3q/KGM/uxM2cbMpP4DqEveXPZfjOPSHnOoGjZm3jguhtw74sEfSrq9Lge4x7/E175+3t4b9Y/kBvoEJ58cCS6paUGnivBoXkv4PaxL2J+0TnpPZU79Q3wv1pRZl75cVd7xHy3e+dFawpssCK5Kdy2oMnJY6jZjxIAR3Hqbsyk4d0aatk4bTC/au8ciua/iP8Z+Tg+2XYSqR1H4tG/f4X5/3gOvxk9AoP69UL37j2QPeBq3HzXRLz91Uw8Ori59p2S1a/hgT9/jcMWyee+Af6s5Qc0BrEnlhu4mUo0k4civhyL343eeWbKIcxRbgu3bYZ2q/OEM/s9NH15VFOdPLis3Gpms5JSnN36T/zh3tewNjDK1x38OGbmPoXb+mWgmuH0PQVV0rrh1j+9gLu7cCu0EhzMfQ9f7vreEjb4AviymC96Yku4I71EDtTJOWH8Ch3ppQk4bVU/qNkX1o8EkOCqTxqZ/cibIc8vjrgSTxb3xaATsWCnCzHjd09jbnEJUjv9Gi9NvAVd0qpGfIQ3U2p2w82/HAItZ8kyfL2qOOozZjL4Avhrth/Ry9qlVZp+btWJV7zzzJRXbKTJvF52QDFT1ljyGJn9cqYX4pl3C8NuqSUGGeqWIusHzmLv7NfxwpKjQGo2fv3kGGRr83czFFZB/S7Z6K1lLcKC9XthxUzfF8DP31wmalmtbPOSd56ZZiSH295xwJwyy8x7/ZBHmP0YKk3Y6mn26//MAkPffHmQkQefCrw4tRLvvvQpzgZuVB8xFqO6p8FQuq/wYPBCSq0GSK8e5macl30BfLEzjgiTFCcvDB/zmneeYSGki14Lty2R7tip0Wq/9pPzwXgL1PWIJA8y8uAj7gePpTi58ivM3EzYd8QtI3qhbiyo50tKzuIMH7cweR748iopzUHFQuZ40TsvWvFlb8Z1u09Ey5609zmHn3pXL31nHTLittnbNR8OWawXgw2dgYzTCaxb+DU0o1/1XsjuWMs4W4SrpUeKsFMDfm1kNa8HGvkqmzwP/G37yhqvLMZWljGsXC9655kpt2isXgu3baZsVueJZvYTgw3NenKHoNNRegDrFm0N/uyVhbY1Y4XceRzfvh6rtTdkoGubujFNE3Q6Qk5ipSLk8cT/lEUsqzTtFOfcGDvPKm5n1Atq9mXvNave7cf3RDL7NaoT5CXLvX1/2SCk86HkKA5sCF6v0aZp7GI+DqHgizyc5AubXI6+HWjaq3zyPPCXbD+mcUGMYpVnCeBl7zwz5b+kaVlQDsNRysxLkixPOLMfNf8iLd9SpmQW1+TjDwJrI2IV00v3zse77+4IvKYq2txyFbpWj1VBIFNQdu5p4HNkFm66IrRUWdHiO/O6d56ZUsvhtg1HKTMvScI8bBsHjpwGF+kYpWhOUcfW7EBRqdGTYa6VBhZUvTUdczm/rzoMvx6eiR+GyRrr5SqxPuCm/Dsl32o5tFS8NPrFOy9a+f0UbjtaWeO5TxMuTZ1b9h0HFaCUKsUAE+l9dPyZFJohtR6ad6kNzAvY8JeuwNrDw9G8rtG4fwZ7/x3w0V/QBQ8/MBDp1QK2/7zX8MdXVwbeWBu9JvwCw5pEd/gJ/Xy4354GvqzYk0NLhStstOt+8c6LVk6KrbRTUyHFcNtObDQSjaZE3ecoznbEqEQMUGLWo5Guz0Y6EnYasuUEKY3RY3BPpM77EiUn/4Vp71yPy37VDTXKSewBd94ts/DwnS/j67OpWLbz95h04/fIfXBa0L13yAQ8898dgt57FjHK08CXQ0hVVrHnJ+88M22D4baXMmhk0Skz2T2fh7qMg0dPa6M4OzuW28wozg6SvOJUklIlBxi2NbaXaVuC83vRiZJJB4+dKQ98XISWV16HYfXy8HHxUSx7/klMavoU7rs6E2n6GvsUVG09HM+8th13jn0Va+c8hbFzgixP7XQ3Xvrf4WhVtVxPEbxZif+eBr4IISWWSMbLB79555nhg6YTCQDfTOM38z635OF0jVNAjuIb9hwH592xjOJcy0C3ZpqG6exUbvS+UMjQ9jL5hg4QSr7CbYcrrONIaTIQD/3xRnzz61wcLFmB3N9cg0/fGoH/GXE5Mts0Qa2q53GqeAc2FgQi+tQKTAMOX1ignzoAD0/+JXqbdu81XwueBv7hk0EvKhFSynyxy+f0m3de+dIZ/5J1IhR3KysxGX/F3qtWj+JmqQ1tL8FIzsHR3/gdF6HJ0AmYOaMxHr0/sK4+sFDnyLL38ELgr2IKrM/vdAUySwqwcF0enrnzUVT9yx9wU2ZtS+z34nueBr6YYwm7tChULEc/eueZKb+sEykKaKrdDHwnRnEzPGOecO1FiPthdSYp1ZE+YBymfTkEi/MWBEJtFaBg2UIsXFuM1IzuuKJja2R0vhQ9e/XVQm39qOgrTBz7G8xYPQt/uLsp2n7wG2TXNFIKmqW8fD5PA18UpWndauI0piNHDL9650VjhAz0zXuPw+79B6LRI+4nahQX3490jNRehM4k0vOBRbaoUqcd+l3Lv9GRszb5CSa8m4sWk/LQ+I7b0ctC0PPDngU+xVORalSL3czBUcTP3nmCN5GO1I1wji90JZHyWn3PTaO4mbKZbS9WRjBOqdEZo57sbIa8mPN4FvhySWWHFPl6pHO/e+dFKru4p+lGAsC3srGKd8tHN4/iMp2RzqO1F6EspYnUC8kXwI+V0ZQWuNKKiaOe23a2jbU88ea3Oty210Zxs3wz015qXOwtKHmLWqmmZOcd6XLUUzZOP8TOi1pQExnkBSY0gcnz/miP+2EUj1ZG3vdre/Es8E+cLotMIAeXiFaZyeKdF40PvN+iUdlKL3akRsD36yhuhj/ME0978YJ51LPAlyvOyMlCvi/Ok807T5Q73FGORkxnl8xKerdx6kApgh2K/O5w33f79Vjaiyw9ub1cpM8XwDfD6FBvq8dGZpp5zJd55FFc2J+5+yv/oiX6qF+aXhM0odK7jQt+6PvvtxRre5H9IrzAC//VWBiuh3pb+WFEClPUcpcrOxf32yhejjkRfvi9vSQF8MN5W0Wod8/dkkfxWH3URWG5D9xV3Zv4dhQX5Yx2TIb24nvgR/K2itYA3HrfylGcK9bERpoEvZGCz618sIMuP7YXIz75GvgcBb3snSdGca744l533+48brgG3Khizc7Fq19U5v9ttLLM6N1+vVaZ9nLytDccd0Td+QL4VMQYafajeVsJJrjhyJGGYbD2HzmlBcegN50ZLzCxXjzeubjMt2QPt12Z9sJoPV5KngW+7J9fMfgBtJ1P3Oid58QoHmsDZKBSrllP5nDbZrzzzPLVC9MlzwI/kn8+weUG77xEjeJmG6jIJ5Y1c5nzVHExiY5uaS9OstyzwI/EpHi8rSK9L9o9N47i0WiW78vLmtlZJYupU/DAivZy4pSa4wt+Gh9LT2Lv8v9g7rwFWLZsGRbMX4sjgWjjdTr2Rs+sTPTI7o8Bg3qjXR3zS21ld9NYvK2MCYx81SujeORSlL8r70BELX8yAd+q9iKWNlPn4oXkIJXncXrXQrz17B/xp4834EJUsQs8CoQiWrsQX/Bv5jQ8W28Y/vDWMxgVIdyQHCJa+O3H6m0VqYK8PopHKlvoPZmXVFLJm0GG5vXTbyvbi+ALA3J4ITkE/FKc3vAufnPTo/giEG8Mab0wYuxw/KRrG9S/mOakszi2Yw0Wf5KL//fFJpQUf4onb2uEjM8nYECYQIOymyhNXUzxelv5cRTXGGLyX7KG2463vRix1e6YBkbfrMw1Z4B/uhBvPfJsAPRA3cGPY9rEm9ElLUSU794bA64ejH4v3I/b/7IIJUVz8Gn+LwNif8Ow5RMRZBhJ1Yy3VTKN4mGZFuaGCB2VLOG2zbSXMKwyvCxMr1bt6GT4EQsvOgD8wI4gs1/HC0sCobIajMGkibcEQF/mNFKuLFWaot+YURg6dRE+OXsKR777vtzt0B8iggwbqwihzDnW+BEdtZ1L47GLszPhe+O1i4fS6JXfIoKM38JtG/GfEp6VsRb5PpG8EpDDfuCX7sC/3/4yIMwHQoZfPxiXhQP9Bc6lpDVGSy4Tj7z/oJY7XGOt98SCC2+LfKD9mqYsbiJJ86BfV5pF5kLwbuO0sl1fvbCe3EyZjPJQ6rPam5MKUZFkRam45saj/cA/vhNrV50wX/bUTIyZ/Q1+XhqISFqjXsTnjHpXIXLJD8qjeOvGNVG/drWk0lzLvAh3LvtFuD3cdrgymLleGe+8cO+Xvfaqe2SJsv3Ar1IVP7wg2R97bzpez66FkZe1R/1qYTbqTamG2o0aB7YJjJ7C9a6auD+oha/Xi0fnTmw5ZG8zN4Xbjq0UkXNb6Z0nf0l2dfaKKTQM+uRiVfK8emdc88s+wX3Bi+fghTHD0KfTlbjhrkfxfzM+QN6SNdh++DRi2T1YUCSboTiqi8RR/82Fge2IAknW/ov76mjMAcFDYZM2zuXNqxTx7fLmFK7OnDp6JdkPfNRCl9sn4tUHhiJD6PRKdmD5v/6GKY/fh7Ej/ws/6dYDV468D09N/RgFe0+a7gQIatFYqZDbOb6P/ptKqvaT8zVtv1cqI9F0asrSABFeM02Z4ZsV3nlG32GHYsWOTkbvtvOaA8APkF+tBQaMmxIwz83DR39/BU89PBbD+3dEHb1kJ7FryQeY8fQ9GJlzC57+926YdYDMbhF0mKBWn2LW2/dm440hLfQ3U3t7x8tLNS2/flGdGHKAlgwmSkxs0H5JVnnnGfGD0YlFopLYK8kZ4Gvc+AGqpWUgs98Q/PzOCZj0xr+wqPAbfDZrOp5//E78uGX1IM9KlmPGA6/g30XmGh53NhWJZhVKAbcOaovlv+qq7QHPe+yRh0/Jx+f5u0VWdTTggBwwUm7QBlk9c8kO7zy58IxhIFJmizri1PVHB4Efygtq7RujXfcBuGb07/Hq7E/w0qiOwUzFBVi9s8xEEvqk/FtW8K3ZfkS/RbfTeQ/3w9PZjbRrHMUGv70OD01f7qvRTC+wBSdywMh49y2wgAxLX2Gld54RYdwkk4kKZa8o9kivbcAv2ToXUyZPxv9NzsWSg9FH75RqrTDoukGS+E/yoidZG52/uaz35ZMc/R++MQtzbuqgj/6cEvR/ZgFWbAq4EapUjgMyL/cdPlXunhd/WO2dZ8QD4Th2bWZk07PRs4m8ZhPwz+O77fPx2ot/xpQXF2FvdNwHeFCK7099d2Fu3xgNQ116I3BJaFMnLNlvmOuqns0w+/7eYDgqJo7+jDM35aN1avQP4Zjgkdc1+1Z754WwSftJ86BIPdsE9SPit9uPNgFfLvYmrN9uwoGndB8WfPRvMGfqgKG4vOVF8ksing/sXObPH24kZ4ipqXf1wvvXtdHfxTjyN724RIvWo19M8pNW9YNmUTGSeZEddnjnGfFh0doi/bKX5vck2ibg/wA1WrRHlma+W4vXn/wr8vae0ZkUelJ6egcWvPJEYD/wLQHU98G9vx2M9JTQXOF/y0xfubW8uB/61PCclsrsF8oU6besLKVizIvJDu88Iz7MWn5Au0yTspfm9yTaJuAHXtzyKtz7i+6a407J6lcxdtgo/O7ld/D5gqUoKCgI/C3DkrxPkPvyI7jtqqG4bdIcHErtgptfeRZ3doltTTOZLuz5ojKMKkpcU2Y/wYmKR7o0i8RYhl5LdnnnhfKBnaJY0DSwQ93Q267/nVIaSHZRWXpiDd5/cjwmvLcqJPBG6BcDEXiyR+P3j9yJa7o0jGtfrzfnbtK3vqYjj9kemFODO98q1CPaUjv71E/bgnqBZEycG6dPXKQVndMiSkheSRTxqbilDodp/X09bdsngKZhWomYaDr2WvAS20Z8MiSlRiaufzYXX7z7In53+/X4cY+MoOsub6Z1RN+f3oJxj0/G6x/lIS93AkbECXq+7sddm/Cgpa9W7BWnUY/K7FeeRXKHKQKclM/h3l92eecZlXjm4qBLOAcKr4Ge5bF1xDdimJ3Xrp20UBO/KPZ/8FDfmD/FXvyRjzfpIwYr9bVRWZ6s2JgLLz1AXwcq96jhp0LUC4neeTnTCzVS7aZbloq47di4n3XwAovK0WjriF/uSw78GN4tqN3n3Cucdj8SGcrsF+SOHG47Er/ccs9u77zQcsoSZU5mg9DbnvjtK+DL4v78NWWmllhqQpn9oG2BLXjG0c3tyW7vvNDyT5kfdP2mZOlFMZ/l8RXwOT8VDii00VdmoUkym/1kN2g5ukwoANzw2wnvPLmcnFII5eGtfZvKtzx17ivgk/Oj+2foFfDZ0l36eTwnyWr2q1+rzHlKji4TDw/tfMYJ77xQ+j9cGlTq8fqALo1Db3vmt++A371dPd0vXwTjqExtJONqP053qNhkkqPLVIaPVj/rlHeeTDc7GuHRyMVf8oajcj4vnPsO+ATquJygDZ5KPopmVqRkM/uJjSGWbD9mBfssf4dT3nky4R98s1P/eX2/dP3ciye+Az4r4erezfW6eO7Tzfp5ZU+SabWfiA8vvNMqyzsrn3fKO0+mmZYD6o2YqNSTVzLK+bxy7kvgUwSjfZUpXtNepApMBrNfaLjtSPxw8h5FfLti50UqR27eNv32g8PKFnrpFz124kvgsw5uHtBSr4rcr3fo51adRDL7ecEEFo0PoeG2o+V36r6T3nmiTKGjfb+sRuKWZ4++Bb486lMhY9VcP7Smjcx+9HWnmcnLSY5gvP/IKVcUhXUoYi7QbMsQa04kv4325Jlvgc/CyaO+lXN9vltOkcx+HC28mKjPECseRXipRJbDae88UVY/jvYsm6+BL4/6nOvbGWwznNlvyPOLbf2uaKB2HN0Ubttp7zzBT35XJD/M7UVZfA18FpKjvrBJcwFOZbz5BNMiHcOZ/Z55t9D2b0eiK557bgm37bR3nuAVrQfCbs/wbn6Y24uy+R74HPXHB7bTYqKrJe2/dicjsx/npl4L8umGcNuJ8M4T7eOh3NXiFGMHtdbP/XDie+Czkob2aq7PV2+bvd2xzTXCmf0YNMRuycOKxpnocNuJ8M4TfKOUIXwYaBr2ut1elEsckwL4HIEfH95elLncNsn6RZtOjMx+7HwY5NPtZj+5sSci3HYivPPYDKjQmzh3u9YiOE2UlcQ2NRPHX5sUwCdXOfcWYbi5s47T5javmv3Eakenw20nwjtPoE9WJDIMGztvv6WkAT4rbvyIjrqij3vqOT3ietHsl4hw24nyzmMboeVHVuj5NfZiUgGfPTd7cJGeeG+NOHXs6DWzXyLCbSfCO48NgAMBLT8i3XP1JeLUd8ekAj5rjz24LPJT0ZaI5BWzn9PhthPlncc2wIFABNngtmuU0Pyakg74WgXf3FkX+aloiyc+nxUNwgtmv/q1y+a38s6wVpQ/9B2J8s4jHRwAxD73HBj8KuILnicl8Ak4Rs8ViXH12egSldjIZo3rqYcN46jDvf3cYPaTRz27w23LSjUnR1xKGRwAmKjFfyIwMPg9JSXwWakUtd8YUubY87u3ViXUtk6Ahe7t5xazn9Ds7yi2b7FOorzzOK+/b+Y6Hed/u6OrtsuyfsGnJ0kLfNYnV3eJRk0x7/U5iZnvy22LZj/uACMWyNCJJNGr/S5ND26rJbTdMr1WnBN8tLIwccSl9cWJROvBuOnf6vN67hwk+y44QUOivpHUwCfT/zy2mz7fZ4QVp+37RhXPxvf2vdl6MBHmITDueHlpQqYkTeuWzfOtNoEm0jvvnmnLde88xtBjp5ssKemBz/k+59diIQ8BZtfa/VgaFeniDi3cl03QRqkkEav97Ay3LXvn0TWWUzAn0pSP1unKPEp9913bwYnPuuYbSQ981gTn15NvKKt4bsXkBvCTNmH2EyZIKv64WaOTq/3sCrcd6p13++AyHwuW3a5E0Iv4eexUnx3VOSnm9TI/FfAvcINLLuePKdP0U+FjtVgrMz6Wc47+k8Z0AzXdYvR3crWf7LJqVbjtRHnncSong57Snly+WOrFy3kV8KXaI/hlTf/wKfmuAT/JTKTZT0gcWw9ao9mXvfOcUqpRihNKRPKTGnzZXMlryZIU8ENqmpp+EaGXYrXbwJ8os58Ity2cXELYFtNPAlCOneeEUo3fFLvpklhKd8miwTeqHAV8A65QqeZm8JNkp81+VoXblr3zOG15bGSmQQ1Ye4kLb0JB76doOvFwSwE/DNeMwO8WhZ8g2Umzn1XhtmXvPC6YslvUpiKPylCRONInO+jJCwV80SIMjqHgd5O2X5DrlNnPinDbTnvnydp78kuBXrQaBfwyToQ5I/iFwo9ZCP5EregLQ6J22W6zHzsYYVGIJ9x2qHeenf7wtBjQ2UnW3tMbUo30ZS1IjfhlvAh7RoWfbOqjD72TdvSwhIXcsNvsJzbS3FAUm2bfyDuPtNqR2MHQI08oIdlZ0WSXzIo8Iz4r4BtxxeAaRwvZi45aaTYwt9j6ZZLtMvuJcNtcP0Awm01OeedxeTWtMAL09MibfX9v2/UIZvngpnwK+DHUBsVpjh5iAQ0bGBtaotbzRyLdDrNfPOG2nfLO4/SLS5lFIA36HXAdRjI650RqF+KeAr7ghMkjAcUFNMKhxU1r542KYKXZL9Zw205459E8yPm8WE9PHlAnQ09Hu6YTRnz22jUF/DhqTMyl6XEmEhueW0V/q8x+8jzZTLhtu73zaF7loiUh2nM+z+mYU5tpirr34lEBvxK1JkZToe1mA0z02vlwxWFnRQuFrKcgvbGu9hPTnGjhtuk0Y5d3HiUJKldpYRGivZjPO7W6LxyfvXJdAb+SNcVRcN7D/cD13CLRH/zaSQvB+a3bUmXNftktamlFytt2LGzRKH6LaLXsFK30zmOHwq3IRKdCIih5MXqRms+HrZIKN1JKA6nCVXUhLg5Qycf4fWIU4kvo+svlpm6cbxJEBKiglyBlLMJIoyadcMRCl+LH+hmCjXNuIX5zRaEVgSvZiU6bu0WPeU/ecpRnp2K39x+/5bekgG9xjVIM5dxWHpEIKG7c6cRilFiLQ3Mkw0oLoPJ5KsduuKKlYWfFzo3acyY6xcjzfl6TOwYqQKlkq0yi9JCbV7aUVryLo7wb+Snoc/tRAd+mGuIIxd1WxcaL/Aznx9xj3Y0eZDJgBa1TxlxaYTRlR0E9BlMo+OR77Ow4BYpX0mEH+tnSXdoedkIi4TfZmTAmnxLryY34kwJ+/Lwz9SQBxQ0Y5cbr1g7AqLMKBTcLnXL/l1rZOY2hwpCJQOVGoKKjoxIx0pRBe8jgXzjAk2fc+DSedxp8JukvKeA70ATYmBnBV/iOi0+KDqB7u3pxj4ziXVYdjWjlXJrhqcQoK+bwsihPBxphS5c7BLN0UaT/ZPEuTJm/u1wnScmBq/is0BOYpSUZ8ingO1jL4earbNzjcprh6t7NdXA5SJbhp0IVlTIAaUoTOozS53+ieS6KeT87Mzo4mRXxKWV8XrC3QqfI71EvMrRXc9PvMiyIumjIAQV8Q7bYe1F0AG8u219udONXOYpe06upK/QAHP0fy11VTpNOs2XzQLhtMbrvHN9Hc1sWUxkjhV8oN/ner1ftx1++2q5PDUQedhy39m2qAC8YYtNRAd8mxpp5LQFABdabC/dUAABHvFt7NEJOZoOEz2tDzX5y2QhUMa830geIvCxrwcZizFtdpEsL4h6PnE6M7p/hig5Ppsuv5wr4LqlZup+GA4XoBLq1TkOi9AFGZj+ZdQQunWjkRMkmf0Mx8jcfNgS7KNe1l6VXsB7I71Hn1nNAAd96nlbqjQIsMxfvKWdbl19KkF1+SRq6tEpD+/Tajs6BQ81+pIsA5qrF6helYu2Oo1i+5TC+WHdIlwRk2nnO6UL/Tg0S1omF0pOMvxXwXVzrHGXXbD+CSJ0Ayae4TVfaDs1qgLvecAMMoYG3o3hUyN0ytWwJLIHPJOb5Rt9UYDfiSuKuKeAnjvcxfZmSgBhNjZSCRi+jZJBWvQoYGrvGxUFwytthGT0jXzt5+hy27DuuXeIW2dwtl9F3xJxezht6nkipJJQW9bsiBxTwK/LEE1coDWzff0ITq7/deTzstMCpwtAawQg9rRvXdHz64VQZ/fQdBXwf1SY7g4NHT2ujdKwjtBk2hEoQTkwrzNCl8sTOAQX82Hnm2Sc4Nw9NhdsO65dqVKsKOX4+b1QPrONXq990FvnmRAHfN1WpCqI4YJ4DKhCHeV6pnIoDvuGAAr5vqlIVRHHAPAcU8M3zSuVUHPANBxTwfVOVqiCKA+Y5oIBvnlcqp+KAbziggO+bqlQFURwwzwF7di40/32V01McKMHhuY+g7x3v4GxUuuuhfU5f9LisPwYPuwp9W9VEStRnVAanOKDs+E5x2hffOYlVL/83rntuRWylSe2G0dNewvgrm0GNNLGxzq7cqh7s4qwf31tahPVLtmolS815AK/+9jIEt9cwKOzZw9ic9zc890oeDpUsx4yHZuDKr36PfjXV7NKAW45fUsB3nOUe/uDxrfh2Kd1+qyNz4EBc0b09IsG4e3YW6hRfj1+9twsoyse3W0+jX5fqHmaAf0iPVG/+KaUqiSUcKNm9EUtP8lWNA0E0GkYEvfbBlHro3LdyG2pYQrh6SQUOKOBXYIm6YMyBcziwvhCaoJ+ahS6taxpnK3f1exTt3hm8UrUD2qVfVO6u+pE4DijgJ473HvvycWxZWYgSUt0mC5c0NDFLPLsVi+Zs0MpZdejl6Fon1WNl9i+5Cvj+rVtrS3b+ADYt26e9s3p2OzSLiuHT2PmvGZi6MjA3SO2Du8fmoIGy51lbJ5V4m4luuxJvV4/6hwNHd2D1Gk7wa6BTZjrCCvqlp3F4x3rkz5mO/332QxxCcwz8w8O4tXNY/b9/eOShkig7vocqK3GkluLM4udw2Y2v4FgMRKS2vAYPPP1bjOqbgWpqtI+Bc/ZnVaK+/Tz2wRe+x97N62ICfUC+R+0WzQPRe+rgIgV617UBJeq7rkrcSNBRbFy+PkhYj3vwyoT+qGdIZinOHd2MBe9MxV8Dm4Qemvcy7j5SH5/9czTaqSHGkGOJuqiAnyjOe+m7Jfuw8Zv9GsW1svsGHHe6I7xhrgeyczqhxoiReJaKvW0HcPhsKdSw764KV/2wu+rDldSU7ijE17toyGuKgd1bRQD9BfKrtkCPK1sEf/ygKqqkKlnfbRWrgO+2GnEdPedxfPt6rNboaoVOLc1o50+ieM/RYEm6NEfDqKY/1xXa9wQp4Pu+iitbwDPYvXEdNE/dmh3Qtll4IV//kjw1yMxQ9nudMe45UcB3T124k5LSg9iQv1mjLbVvFlpfbEJsP7QL6/dyalA74NrbGD90Z8mSmioF/KSufhOFP7UDKxcWBzJWRauel6ChCdyX7N2KFcQ9GiEzI00F4DDBZqezKOA7zXGPfe98YEVewXES3QS9Aivyok/Xz+O7QwFNvlbOBmhUV433bqxyBXw31opraCpB8cZVWKPR0xGXtq1tgrLzOHnkcFAngG/xj09W4YSJp1QWZzmgXHad5bf6muKAKzigRnxXVIMiQnHAWQ4o4DvLb/U1xQFXcEAB3xXVoIhQHHCWAwr4zvJbfU1xwBUc+P/ydTYxWjMWfAAAAABJRU5ErkJggg==\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\n### Problem ID: 1530\n\n**Problem:**\n&lt;image&gt;$P Q R S$ is a rhombus inscribed in a circle. Find $m \\angle Q R P$ \n\n**Choices:**\n30, 45, 60, 90\n\n**Answer (Index):** 45\n\n**Ground Truth Label:** B\n"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# Extract and print all IDs from the dataset\nall_ids = [row['id'] for row in dataset]\nprint(all_ids)\n","metadata":{"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"VwbfDQdlOAmB","outputId":"5439b63c-9e50-44f4-cf4d-d5bbb60efd97","execution":{"iopub.status.busy":"2025-04-08T18:41:05.156197Z","iopub.execute_input":"2025-04-08T18:41:05.156615Z","iopub.status.idle":"2025-04-08T18:41:16.652180Z","shell.execute_reply.started":"2025-04-08T18:41:05.156583Z","shell.execute_reply":"2025-04-08T18:41:16.650812Z"}},"outputs":[{"name":"stdout","text":"[84, 2561, 1318, 208, 2616, 1221, 2098, 692, 185, 2876, 2931, 1530, 2649, 2675, 1333, 979, 234, 2767, 1882, 650, 209, 1119, 1336, 2041, 187, 1509, 695, 2929, 1664, 1933, 1366, 969, 2343, 790, 2246, 154, 1201, 2315, 984, 1545, 490, 1627, 2247, 2037, 2286, 2721, 2728, 2539, 2509, 2007, 1849, 2848, 2266, 2100, 623, 166, 2584, 2210, 436, 68, 972, 2853, 778, 894, 1992, 1915, 1076, 1458, 2633, 1448, 1114, 570, 2748, 1096, 1526, 2778, 1203, 409, 263, 1953, 1052, 1913, 76, 974, 1610, 483, 1706, 405, 1703, 1744, 81, 889, 2020, 2307, 485, 2523, 83, 353, 376, 1493, 1397, 905, 2222, 1578, 311, 1864, 785, 1127, 608, 1940, 2021, 2925, 299, 141, 1894, 2146, 2390, 1814, 1314, 2211, 978, 337, 1045, 1354, 1316, 2796, 2713, 2950, 2858, 2617, 170, 301, 2253, 1690, 1515, 1718, 163, 966, 851, 861, 2077, 451, 2807, 2685, 1474, 2104, 295, 1112, 1754, 2085, 2382, 186, 1154, 1062, 1430, 1429, 2814, 1288, 1614, 2659, 2798, 1668, 2916, 2330, 630, 2151, 2923, 267, 2864, 2756, 1710, 886, 1632, 1550, 1097, 57, 2890, 1131, 134, 98, 2934, 2803, 1656, 2672, 2005, 858, 1715, 1481, 1786, 403, 1516, 577, 1551, 2600, 2381, 548, 1544, 1369, 2939, 121, 1792, 891, 2424, 2813, 1180, 1700, 2991, 2456, 677, 2928, 950, 1299, 1950, 2984, 1303, 33, 2186, 2610, 1820, 1214, 872, 2223, 1040, 655, 2527, 1281, 1986, 1056, 1858, 2327, 1067, 2736, 2619, 1670, 24, 120, 827, 1305, 2184, 489, 2204, 16, 2882, 2400, 1167, 1124, 2167, 215, 2740, 1358, 2256, 1963, 537, 1809, 740, 2621, 1178, 909, 1289, 2460, 2510, 2914, 468, 1542, 77, 1752, 826, 2822, 747, 477, 1466, 2885, 1206, 2472, 1334, 2759, 135, 898, 1061, 1980, 1491, 1941, 2412, 921, 764, 255, 1935, 156, 583, 1213, 478, 515, 2218, 784, 1200, 2771, 1404, 2963, 2321, 1094, 2637, 791, 2962, 1901, 1554, 1235, 732, 1489, 1988, 67, 2766, 324, 1002, 111, 197, 2069, 1363, 476, 2612, 2958, 1400, 2712, 341, 2760, 1407, 2974, 380, 2591, 2937, 2682, 1311, 1113, 2108, 221, 2450, 1508, 510, 1353, 1322, 700, 2455, 2081, 1465, 1740, 138, 2034, 1132, 2634, 2185, 2187, 624, 2496, 752, 1283, 1442, 258, 1251, 2427, 2542, 636, 2908, 2169, 867, 2754, 2751, 1789, 2607, 2915, 665, 1832, 2379, 331, 2018, 1659, 1098, 2444, 653, 544, 1414, 2261, 2464, 400, 1041, 641, 2191, 1617, 1473, 2053, 663, 1974, 2043, 207, 2874, 459, 1275, 1559, 2836, 1231, 1699, 957, 975, 261, 987, 629, 2443, 2149, 1760, 2698, 788, 865, 1155, 786, 463, 2926, 2105, 1619, 2295, 2470, 1171, 971, 361, 2026, 246, 2179, 2267, 1444, 2606, 1483, 2358, 2641, 404, 91, 2490, 2477, 1908, 2369, 1816, 229, 1965, 682, 1123, 1507, 2745, 2679, 840, 1648, 926, 352, 1224, 1621, 1218, 2628, 2341, 2259, 2856, 2689, 2448, 1207, 47, 576, 20, 1362, 2758, 1539, 2092, 323, 2532, 776, 56, 2048, 935, 2743, 1095, 1427, 1309, 831, 1848, 545, 244, 1802, 97, 1368, 946, 931, 1477, 242, 69, 2123, 1653, 2304, 1657, 819, 1010, 2973, 2154, 402, 2802, 109, 1436, 2786, 1128, 997, 2781, 1088, 654, 805, 1779, 2695, 2507, 1997, 1783, 2573, 2879, 164, 954, 398, 838, 284, 1486, 896, 2495, 2447, 1702, 424, 587, 2332, 2345, 1145, 59, 1269, 262, 280, 1697, 1645, 2564, 2058, 1989, 2706, 2512, 781, 2212, 1413, 915, 1528, 1402, 1391, 1644, 14, 955, 1776, 1833, 2888, 518, 2835, 1209, 60, 713, 1367, 2252, 1513, 1781, 2353, 1912, 316, 1212, 2658, 1777, 698, 2887, 1036, 1153, 2795, 202, 596, 1883, 675, 2850, 763, 1772, 517, 615, 1872, 2244, 1332, 2889, 326, 2226, 614, 2693, 2871, 82, 2620, 2014, 2785, 2582, 194, 2325, 1889, 2768, 239, 1307, 890, 282, 1476, 294, 2371, 189, 730, 768, 556, 554, 9, 1078, 1468, 2541, 1142, 613, 2297, 2044, 961, 1720, 2711, 1628, 433, 1325, 2664, 1089, 1867, 1859, 2883, 2959, 18, 2727, 458, 970, 1934, 1157, 151, 2627, 2485, 708, 679, 2799, 1274, 2236, 1185, 2662, 2919, 593, 1503, 832, 670, 825, 1979, 737, 2886, 1405, 1049, 1384, 722, 173, 1151, 440, 2194, 1612, 1234, 2557, 1173, 938, 1223, 2920, 661, 2990, 2417, 127, 2770, 2910, 2421, 836, 1647, 733, 1592, 227, 1978, 2141, 602, 2338, 1168, 558, 2725, 2579, 1365, 1133, 1293, 549, 214, 130, 2309, 1548, 1678, 415, 1162, 646, 407, 794, 2783, 532, 2670, 924, 855, 162, 1412, 908, 1177, 2189, 1643, 313, 428, 1904, 2331, 1604, 1219, 453, 2536, 183, 795, 362, 1490, 2851, 1488, 1315, 137, 44, 2377, 342, 198, 1835, 1044, 363, 1887, 1855, 2701, 743, 881, 2545, 1594, 2292, 2952, 2001, 302, 1778, 243, 906, 1660, 912, 1186, 620, 2769, 1836, 1607, 2680, 1359, 813, 1379, 2703, 2101, 2483, 2585, 2823, 1364, 2453, 105, 2284, 3, 634, 1698, 2228, 1324, 269, 1242, 2842, 1932, 552, 1939, 2936, 1461, 1372, 2838, 2481, 839, 2022, 373, 2733, 1828, 2789, 291, 1140, 1462, 673, 1406, 672, 1831, 1959, 1174, 357, 1834, 2609, 706, 1862, 2644, 852, 2281, 610, 2780, 2636, 1271, 422, 2180, 1795, 435, 493, 2630, 312, 2982, 1120, 1750, 1787, 512, 799, 455, 1878, 1408, 2878, 1025, 2753, 2750, 2153, 1196, 1176, 2726, 1927, 439, 1453, 2047, 1072, 2373, 947, 1595, 2363, 897, 547, 2220, 2587, 1860, 2529, 1538, 846, 2902, 421, 694, 753, 205, 1066, 1019, 2491, 473, 723, 919, 1105, 1257, 2242, 1942, 600, 854, 318, 2809, 2593, 1840, 50, 1922, 2652, 460, 1249, 815, 240, 647, 1981, 649, 710, 2027, 314, 2111, 1082, 1399, 2947, 1753, 2862, 541, 86, 2731, 106, 1591, 158, 2061, 1460, 171, 2540, 1888, 1905, 804, 959, 2967, 1683, 2554, 2160, 2906, 2583, 2676, 2690, 1357, 1791, 2599, 528, 2543, 2665, 531, 2964, 775, 372, 2442, 360, 420, 2173, 2138, 603, 2238, 2116, 1001, 749, 1654, 1449, 2734, 2893, 2473, 1126, 1679, 845, 578, 2008, 586, 2118, 1626, 2383, 2462, 392, 2479, 717, 927, 731, 1926, 1115, 1457, 497, 914, 1947, 1684, 2420, 2478, 2945, 2196, 2303, 410, 2482, 2710, 1762, 2357, 837, 1581, 521, 1812, 771, 1233, 2164, 502, 1230, 338, 2755, 1520, 1327, 500, 2718, 1599, 2316, 54, 1803, 2152, 1306, 2060, 1579, 2742, 1047, 256, 756, 133, 2661, 238, 1245, 1385, 1998, 144, 1378, 1596, 223, 2428, 1122, 484, 2899, 765, 928, 1184, 2828, 177, 2715, 2264, 1675, 1199, 1756, 1570, 46, 2837, 503, 1817, 1431, 2426, 1423, 607, 1418, 2440, 1827, 2667, 1552, 1553, 2951, 2938, 2171, 644, 2946, 2176, 1956, 1851, 222, 1586, 1890, 472, 1138, 61, 2776, 562, 1680, 2827, 1480, 339, 2070, 2961, 167, 2434, 1598, 895, 1478, 1417, 2684, 1938, 990, 1499, 26, 2139, 2459, 2894, 2301, 930, 697, 2655, 1331, 965, 321, 1498, 2414, 1902, 551, 2432, 2575, 2763, 1877, 631, 2130, 1026, 760, 572, 2190, 2550, 283, 2918, 2374, 2452, 2023, 822, 1897, 1256, 1880, 1558, 2805, 2537, 2757, 2076, 119, 1790, 1736, 1146, 340, 0, 911, 1191, 774, 1479, 2861, 2779, 536, 584, 829, 1533, 2318, 847, 1086, 1837, 1991, 1035, 2366, 888, 1093, 1166, 1014, 2346, 1757, 1023, 1696, 2181, 2596, 2337, 2707, 1611, 87, 994, 2775, 396, 2323, 99, 1148, 1237, 812, 481, 1034, 1691, 942, 968, 344, 2855, 2854, 1891, 2206, 868, 1751, 1337, 2560, 39, 2355, 674, 884, 285, 1865, 1541, 2904, 2419, 1949, 2168, 1755, 2255, 2458, 2489, 1721, 266, 148, 1137, 1723, 2050, 604, 464, 345, 1005, 925, 4, 1731, 1401, 2042, 1804, 2474, 2841, 1037, 501, 1635, 426, 1433, 1841, 1523, 1583, 146, 714, 30, 17, 1238, 1204, 2598, 590, 1004, 2860, 857, 2423, 2681, 2872, 686, 1360, 1639, 2826, 461, 412, 1344, 444, 1416, 2192, 1121, 2408, 1338, 1070, 982, 2028, 2273, 1189, 96, 482, 450, 1518, 2508, 140, 2348, 2350, 2547, 1463, 64, 1492, 671, 2109, 2875, 609, 524, 1952, 1987, 1227, 2395, 1665, 1348, 2578, 933, 272, 2930, 2328, 2820, 251, 2719, 2407, 343, 2177, 2080, 1012, 989, 2035, 469, 174, 879, 579, 1183, 2765, 1018, 2439, 2342, 1217, 2198, 2677, 2966, 2896, 2030, 2296, 1722, 142, 1842, 2291, 1874, 1075, 932, 157, 414, 1774, 1510, 538, 315, 1829, 102, 2640, 2312, 625, 2784, 2054, 1954, 1250, 188, 496, 2308, 217, 354, 2744, 249, 1000, 2570, 2441, 2556, 1729, 5, 2142, 1692, 1051, 1347, 2344, 2046, 2454, 45, 471, 1381, 448, 1593, 1222, 1620, 2233, 853, 1531, 1410, 1946, 2465, 2504, 621, 2333, 1080, 2626, 305, 617, 93, 1220, 2656, 882, 419, 1970, 85, 1099, 1805, 1470, 2263, 2516, 288, 199, 2015, 1801, 1971, 1188, 1475, 1192, 2389, 1467, 1441, 2631, 823, 748, 2201, 2867, 1396, 136, 948, 1662, 1375, 2038, 648, 1999, 12, 1484, 1106, 1308, 2999, 438, 1852, 762, 2091, 1798, 1073, 1807, 2095, 1439, 1677, 2692, 1081, 232, 1376, 2392, 2006, 688, 1519, 397, 2310, 165, 2352, 1469, 2942, 783, 820, 1688, 1535, 622, 1704, 63, 273, 770, 669, 101, 2372, 2010, 1967, 192, 1588, 1340, 13, 179, 306, 1411, 1039, 1374, 2800, 139, 1589, 1502, 1651, 1210, 581, 1540, 1022, 1663, 664, 1159, 253, 745, 651, 1973, 1071, 810, 1321, 2314, 114, 2287, 1714, 1046, 1285, 2229, 1602, 2411, 195, 559, 1394, 2830, 2535, 1387, 1208, 1695, 371, 2538, 2249, 1504, 2360, 128, 79, 2157, 2062, 2089, 181, 2257, 505, 1259, 1370, 427, 582, 1687, 248, 92, 506, 132, 304, 1784, 1248, 580, 1584, 1652, 2831, 1839, 1494, 466, 333, 2403, 2115, 2499, 2857, 2898, 2241, 1846, 1734, 1228, 2716, 803, 601, 2819, 1280, 1341, 2708, 963, 2467, 2306, 2998, 520, 2375, 1923, 298, 2502, 2051, 657, 211, 1689, 2429, 598, 1181, 219, 116, 275, 2747, 1761, 153, 2844, 871, 689, 117, 1582, 1326, 1058, 201, 2387, 973, 1727, 2430, 2697, 983, 1063, 401, 2059, 2122, 1290, 1069, 2117, 320, 2461, 1624, 378, 2955, 874, 1818, 180, 335, 2569, 1032, 1681, 334, 150, 1232, 1211, 2188, 2057, 2072, 72, 2299, 1383, 976, 2326, 1886, 2125, 1975, 2980, 1799, 1819, 2172, 1566, 2525, 1169, 1198, 1027, 327, 416, 2590, 1881, 2469, 2773, 1033, 1873, 379, 2174, 474, 1797, 19, 399, 1600, 1276, 75, 2592, 2562, 2488, 1565, 49, 642, 543, 2559, 1501, 1043, 368, 693, 2073, 1254, 1899, 241, 2237, 297, 1435, 2700, 1455, 37, 2240, 999, 1471, 470, 2422, 1709, 2524, 1943, 23, 1672, 1773, 1085, 1136, 2067, 2438, 2135, 443, 2221, 2714, 1091, 29, 462, 2131, 1011, 2884, 2175, 2294, 2386, 2604, 1267, 1, 330, 1708, 1464, 309, 2012, 589, 1928, 2787, 1298, 2347, 332, 2064, 1717, 125, 62, 2300, 41, 1995, 356, 2791, 514, 1390, 568, 1577, 1284, 1524, 833, 2205, 2567, 1345, 147, 2749, 1260, 2335, 2227, 2812, 1822, 2873, 1068, 1149, 1937, 2818, 375, 53, 916, 535, 48, 1969, 2019, 2530, 626, 952, 1590, 434, 1637, 8, 2551, 876, 1016, 557, 2497, 1613, 1972, 1707, 1985, 289, 811, 200, 1190, 2288, 1108, 796, 1618, 1573, 2361, 2594, 1893, 2225, 159, 2993, 841, 274, 66, 1868, 94, 22, 2503, 206, 1330, 2103, 2311, 1903, 2083, 1682, 1737, 1342, 660, 457, 1116, 2505, 2995, 383, 707, 1253, 2825, 2646, 2868, 370, 1065, 2163, 2224, 1895, 2239, 2364, 1569, 571, 1054, 2493, 1884, 1083, 2158, 1788, 1193, 2572, 2602, 1930, 1264, 1766, 2313, 1236, 2953, 2686, 2457, 2365, 2891, 632, 734, 507, 1769, 2045, 526, 618, 565, 2806, 844, 1255, 1719, 2653, 1605, 2932, 917, 2319, 920, 899, 2356, 310, 2691, 2629, 2870, 2274, 848, 691, 2215, 2941, 1646, 2298, 678, 1443, 806, 704, 1924, 1346, 2232, 1165, 1156, 2096, 728, 2632, 1029, 184, 1205, 1960, 1641, 758, 1549, 2511, 699, 216, 2276, 1642, 2648, 1571, 1243, 1393, 2717, 432, 2144, 995, 2280, 1633, 1021, 418, 2586, 944, 2988, 798, 533, 2397, 1454, 2024, 425, 390, 680, 800, 2162, 1532, 2339, 2996, 110, 2673, 2399, 1277, 25, 2623, 160, 1409, 701, 681, 696, 1961, 2406, 1215, 2004, 3000, 1993, 1511, 115, 124, 1866, 1785, 1456, 2084, 2127, 2852, 385, 934, 1512, 2396, 2737, 2965, 567, 447, 1194, 1287, 1771, 807, 271, 300, 277, 2376, 887, 2290, 1735, 2834, 1574, 1534, 550, 1352, 1853, 467, 996, 958, 782, 1109, 329, 834, 1977, 2235, 645, 1733, 2678, 100, 736, 1134, 606, 2143, 873, 1272, 437, 1268, 2289, 1006, 2216, 1732, 38, 1101, 2129, 252, 247, 2881, 123, 2669, 1844, 2320, 2065, 2576, 1861, 1377, 2165, 161, 619, 1403, 394, 1775, 2729, 2036, 108, 2546, 2625, 2601, 118, 328, 1013, 1163, 1958, 683, 1567, 2688, 964, 569, 2025, 2577, 2514, 2845, 1135, 169, 759, 534, 1050, 480, 155, 2816, 2132, 2494, 2501, 504, 922, 1059, 2293, 2368, 2394, 1587, 1428, 522, 1064, 2078, 1898, 2120, 1247, 2017, 351, 2694, 1187, 11, 2055, 1863, 492, 2824, 1292, 1265, 70, 703, 821, 442, 1038, 1546, 875, 2588, 1110, 1742, 2647, 1976, 73, 564, 1125, 2608, 998, 2102, 2553, 1179, 850, 2746, 1765, 2398, 949, 2549, 292, 1676, 2269, 937, 90, 2384, 1616, 279, 1527, 1808, 55, 2986, 2978, 446, 2401, 862, 317, 1606, 627, 719, 2971, 1139, 1693, 2248, 1948, 1459, 628, 1437, 1857, 2976, 2702, 525, 904, 527, 1572, 1361, 539, 417, 1425, 2761, 2517, 1876, 2635, 1452, 1100, 2804, 2571, 2580, 411, 892, 835, 724, 2597, 992, 2213, 325, 393, 2979, 1279, 1962, 712, 491, 1556, 893, 1850, 1373, 870, 2903, 2128, 1767, 1560, 1172, 2178, 231, 1445, 413, 711, 2231, 1294, 1104, 2970, 1239, 2668, 2155, 265, 1838, 2500, 1728, 2568, 2074, 475, 1487, 2413, 2016, 814, 2094, 1917, 843, 2251, 2484, 2066, 2948, 2892, 2124, 2099, 726, 319, 245, 986, 2913, 364, 2520, 2832, 1371, 883, 860, 901, 2683, 1421, 2031, 705, 1537, 2613, 1055, 2657, 1229, 1879, 1240, 1885, 943, 1871, 1823, 1525, 2068, 1919, 2997, 2195, 2260, 1741, 34, 1273, 824, 1920, 2393, 2202, 1424, 1102, 2182, 2960, 1438, 1746, 2000, 1918, 2574, 1432, 381, 1568, 1335, 939, 2643, 1244, 2815, 1713, 1666, 2989, 1392, 1060, 107, 2741, 348, 2436, 203, 849, 2987, 789, 250, 816, 802, 2709, 1640, 2552, 1202, 2475, 104, 1087, 204, 2359, 2262, 597, 2720, 36, 1603, 863, 1225, 878, 1780, 666, 2418, 553, 2378, 2943, 877, 766, 960, 2810, 687, 599, 809, 2651, 406, 2940, 575, 1395, 220, 1286, 2486, 454, 1725, 2230, 793, 2969, 2199, 281, 2687, 145, 225, 2521, 2305, 1907, 611, 1323, 1398, 2161, 408, 254, 2013, 1563, 2380, 2581, 126, 772, 2533, 1020, 129, 1278, 1630, 2663, 1638, 977, 2040, 951, 1447, 702, 1529, 1084, 801, 900, 32, 1758, 2087, 1743, 391, 667, 751, 2764, 1380, 1517, 1130, 2595, 2654, 2063, 1295, 1266, 1856, 684, 495, 1030, 1349, 588, 2794, 2666, 818, 2334, 1892, 2924, 2843, 1966, 2788, 386, 1655, 546, 6, 566, 2528, 2722, 2367, 1821, 1415, 51, 430, 656, 2476, 479, 2052, 560, 2245, 1304, 1150, 1726, 445, 43, 149, 2642, 2833, 767, 918, 42, 2901, 1782, 1350, 2792, 1990, 594, 7, 307, 1008, 131, 1031, 1909, 21, 769, 2107, 2732, 1658, 2730, 1724, 1763, 1143, 1117, 2907, 2362, 1107, 346, 797, 885, 2385, 2388, 1547, 2410, 367, 1955, 1557, 913, 58, 715, 1667, 1623, 903, 349, 523, 830, 2735, 755, 2513, 2674, 780, 787, 2049, 1144, 1661, 2801, 1003, 1197, 658, 1951, 2774, 1983, 1079, 1426, 2451, 595, 1825, 2258, 2797, 953, 720, 1634, 190, 1241, 940, 2270, 2877, 1575, 659, 103, 1674, 2638, 74, 2391, 1686, 2895, 2009, 2082, 35, 235, 509, 499, 1748, 80, 2869, 923, 591, 1964, 1028, 2402, 1562, 1870, 956, 2317, 2370, 2515, 465, 2793, 2150, 168, 1355, 637, 1636, 2029, 449, 633, 95, 1440, 2214, 2624, 2849, 2696, 2433, 2611, 1446, 2977, 2234, 638, 2340, 1830, 1007, 308, 1329, 1896, 1608, 2445, 1900, 2208, 2790, 1450, 52, 2808, 1768, 1671, 1301, 1813, 1152, 779, 640, 2949, 2994, 355, 2322, 122, 1170, 388, 1103, 2200, 2250, 1118, 2956, 2555, 2897, 2449, 1182, 389, 2466, 2933, 423, 808, 842, 2282, 237, 2480, 1597, 1929, 374, 2463, 2909, 2618, 985, 236, 738, 1931, 2278, 1875, 1482, 1246, 2724, 2975, 2183, 2603, 2003, 2137, 2566, 1564, 230, 268, 746, 869, 1328, 31, 1738, 287, 2859, 2645, 336, 635, 910, 563, 2145, 2268, 859, 1669, 1015, 2088, 2415, 1854, 196, 377, 2847, 1226, 2492, 2409, 2944, 1561, 1957, 2302, 2075, 988, 1302, 1845, 1389, 817, 2416, 929, 643, 2354, 754, 511, 15, 1601, 1522, 278, 2900, 1984, 395, 2983, 2254, 652, 2954, 2487, 176, 2968, 2992, 991, 10, 1543, 1673, 2563, 2518, 980, 2071, 1077, 1296, 739, 2243, 2506, 1764, 2589, 1968, 2148, 27, 2079, 2134, 2471, 1711, 1017, 1356, 508, 1216, 2846, 1810, 792, 1009, 1705, 2912, 1730, 1982, 1944, 2671, 676, 286, 2351, 2752, 182, 1300, 1815, 727, 2817, 431, 2863, 555, 226, 2207, 2121, 529, 1451, 1175, 1129, 487, 2519, 1555, 1911, 757, 2336, 561, 668, 936, 2921, 1434, 2217, 1793, 2738, 2090, 2704, 2972, 864, 1057, 2275, 2283, 1826, 1796, 639, 1164, 2404, 1382, 941, 2981, 2699, 366, 2650, 322, 1685, 1759, 2056, 573, 2622, 193, 228, 1712, 2829, 612, 616, 233, 856, 2922, 1609, 1495, 902, 1745, 967, 2705, 2985, 1910, 2762, 1580, 290, 1536, 2166, 1996, 1794, 1650, 540, 2110, 519, 1339, 3001, 1092, 2526, 1351, 945, 2133, 382, 2522, 456, 2865, 1320, 1388, 662, 993, 2285, 498, 1843, 1811, 1506, 1024, 1505, 441, 1869, 1312, 741, 773, 1048, 709, 488, 28, 1258, 725, 1291, 516, 1925, 1521, 113, 2097, 1090, 1649, 387, 2821, 981, 486, 2140, 592, 2086, 1631, 1042, 2935, 1945, 71, 585, 143, 191, 1576, 2112, 828, 2170, 2905, 2880, 2782, 88, 2114, 513, 685, 347, 542, 1147, 2405, 2106, 2147, 1420, 1994, 2548, 276, 218, 2840, 2032, 2772, 2209, 1310, 716, 2531, 2279, 1074, 270, 212, 777, 1111, 1422, 2446, 1906, 2033, 2723, 1317, 1770, 2329, 2468, 761, 1847, 2159, 1716, 2739, 1282, 452, 1195, 1485, 1419, 1625, 721, 1806, 369, 2126, 907, 257, 1262, 574, 260, 2011, 742, 350, 1936, 690, 112, 2265, 2039, 1053, 1701, 293]\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Examine the dataset structure\nprint(\"Dataset type:\", type(dataset))\nprint(\"First item type:\", type(dataset[0]))\nprint(\"Keys in first item:\", dataset[0].keys() if hasattr(dataset[0], 'keys') else \"Not a dictionary\")\n\n# If the dataset is a string or another format, let's see a sample\nprint(\"Sample from dataset:\", dataset[0])","metadata":{"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"QMQKxKOwOAmC","outputId":"2c2306fb-b128-42b1-bf27-7941bd415553","execution":{"iopub.status.busy":"2025-04-08T18:41:16.654768Z","iopub.execute_input":"2025-04-08T18:41:16.655112Z","iopub.status.idle":"2025-04-08T18:41:16.705998Z","shell.execute_reply.started":"2025-04-08T18:41:16.655082Z","shell.execute_reply":"2025-04-08T18:41:16.705179Z"}},"outputs":[{"name":"stdout","text":"Dataset type: <class 'datasets.arrow_dataset.Dataset'>\nFirst item type: <class 'dict'>\nKeys in first item: dict_keys(['images', 'problem', 'answer', 'id', 'choices', 'ground_truth'])\nSample from dataset: {'images': [<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=648x405 at 0x7B7137A5E770>], 'problem': '<image>Find $x$.', 'answer': '2', 'id': 84, 'choices': ['1', '2', '3', '4'], 'ground_truth': 'B'}\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"pip install -U bitsandbytes\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T18:41:16.706998Z","iopub.execute_input":"2025-04-08T18:41:16.707261Z","iopub.status.idle":"2025-04-08T18:41:20.257670Z","shell.execute_reply.started":"2025-04-08T18:41:16.707240Z","shell.execute_reply":"2025-04-08T18:41:20.256737Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.45.5)\nRequirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import os\nimport io\nimport torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom datasets import Dataset, DatasetDict\nfrom torch.utils.data import DataLoader, Dataset as TorchDataset\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Disable Weights & Biases logging\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\nfrom transformers import (\n    LlavaForConditionalGeneration, \n    LlavaProcessor, \n    BitsAndBytesConfig,\n    TrainingArguments,\n    Trainer\n)\nfrom peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n\n##############################################\n# Dataset loading and splitting\n##############################################\ndef load_geometry3k_dataset():\n    \"\"\"\n    Loads the Geometry3K dataset from the Arrow file in the current directory.\n    :return: A Hugging Face Dataset object.\n    \"\"\"\n    arrow_file_path = \"/kaggle/input/geometry3k/data-00000-of-00001.arrow\"\n\n    # Load the dataset from the Arrow file\n    dataset = Dataset.from_file(arrow_file_path)\n    return dataset\n\ndef get_train_val_test_splits(dataset):\n    \"\"\"\n    Splits the dataset into train, validation, and test sets.\n    - Train: first 2101 samples\n    - Validation: next 300 samples\n    - Test: remaining samples\n    \"\"\"\n    total_samples = dataset.num_rows\n    train_end = min(2101, total_samples)\n    val_end = min(train_end + 300, total_samples)\n\n    train_dataset = dataset.select(range(0, train_end))\n    val_dataset   = dataset.select(range(train_end, val_end))\n    test_dataset  = dataset.select(range(val_end, total_samples))\n\n    return {\n        \"train\": train_dataset,\n        \"validation\": val_dataset,\n        \"test\": test_dataset\n    }\n\n# Load and split the dataset\ndataset = load_geometry3k_dataset()\ndataset_splits = get_train_val_test_splits(dataset)\nprint(\"Train samples:\", len(dataset_splits[\"train\"]))\nprint(\"Validation samples:\", len(dataset_splits[\"validation\"]))\nprint(\"Test samples:\", len(dataset_splits[\"test\"]))\nprint(\"First training sample:\")\nprint(dataset_splits[\"train\"][0])\n\n##############################################\n# Data Preparation\n##############################################\ndef prepare_dataset(dataset_split):\n    prepared_data = []\n    \n    for idx, item in enumerate(dataset_split):\n        # Check if images are present\n        if not item['images']:\n            continue\n            \n        # Process each image in the item\n        for img in item['images']:\n            # Convert image data to PIL Image based on its format\n            if isinstance(img, bytes):\n                img = Image.open(io.BytesIO(img))\n            elif isinstance(img, str):\n                img = Image.open(img)\n            elif isinstance(img, np.ndarray):\n                img = Image.fromarray(img)\n                \n            # Create an entry for this image\n            entry = {\n                'image': img,\n                'problem_text': item['problem'],\n                'choices': item['choices'],\n                'ground_truth': item['ground_truth'],\n                'answer': item['answer'],\n                'id': item['id']\n            }\n            prepared_data.append(entry)\n    \n    return Dataset.from_list(prepared_data)\n\n# Prepare splits for training, validation, and test\ntrain_data = prepare_dataset(dataset_splits[\"train\"])\nval_data = prepare_dataset(dataset_splits[\"validation\"])\ntest_data = prepare_dataset(dataset_splits[\"test\"])\n\nprocessed_dataset = DatasetDict({\n    'train': train_data,\n    'validation': val_data,\n    'test': test_data\n})\n\n##############################################\n# Load model and processor\n##############################################\nmodel_name = \"llava-hf/llava-1.5-7b-hf\"  # Adjust model choice if needed\n\nprocessor = LlavaProcessor.from_pretrained(model_name)\nbnb_config = BitsAndBytesConfig(\n    load_in_8bit=True,  # Alternatively load_in_4bit=True if desired\n    llm_int8_threshold=6.0,\n    llm_int8_skip_modules=None,\n    llm_int8_enable_fp32_cpu_offload=True\n)\n\nmodel = LlavaForConditionalGeneration.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n)\n\nmodel = prepare_model_for_kbit_training(model)\n\n##############################################\n# Create custom dataset class for training\n##############################################\nclass GeometryDataset(TorchDataset):\n    def __init__(self, dataset, processor):\n        self.dataset = dataset\n        self.processor = processor\n    \n    def __len__(self):\n        return len(self.dataset)\n    \n    def __getitem__(self, idx):\n        item = self.dataset[idx]\n        # Construct prompt for the problem\n        prompt = f\"\"\"\n        Please solve this geometry problem.\n        \n        Problem: {item['problem_text']}\n        \n        Options:\n        A: {item['choices'][0]}\n        B: {item['choices'][1]}\n        C: {item['choices'][2]}\n        D: {item['choices'][3]}\n        \n        The correct answer is: \n        \"\"\"\n        inputs = self.processor(\n            text=prompt,\n            images=item['image'],\n            return_tensors=\"pt\",\n            padding=\"max_length\",\n            max_length=512,\n            truncation=True\n        )\n        \n        target_text = item['ground_truth']\n        target = self.processor.tokenizer(\n            target_text,\n            return_tensors=\"pt\",\n            padding=\"max_length\",\n            max_length=8,\n            truncation=True\n        )\n        \n        # Squeeze dimensions\n        for k, v in inputs.items():\n            if isinstance(v, torch.Tensor):\n                inputs[k] = v.squeeze()\n        \n        inputs[\"labels\"] = target[\"input_ids\"].squeeze()\n        return inputs\n\n##############################################\n# Data collator function\n##############################################\ndef collate_fn(batch):\n    input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n    attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n    labels = torch.stack([item[\"labels\"] for item in batch])\n    \n    if \"pixel_values\" in batch[0]:\n        pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\": labels,\n            \"pixel_values\": pixel_values\n        }\n    else:\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\": labels\n        }\n\n##############################################\n# Apply LoRA for efficient fine-tuning\n##############################################\npeft_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,\n    r=16,  # Rank\n    lora_alpha=32,\n    lora_dropout=0.1,\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    bias=\"none\",\n)\n\npeft_model = get_peft_model(model, peft_config)\ntrainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in peft_model.parameters())\nprint(f\"Trainable parameters: {trainable_params}\")\nprint(f\"Total parameters: {total_params}\")\nprint(f\"Percentage of trainable parameters: {100 * trainable_params / total_params:.2f}%\")\n\n##############################################\n# Training Arguments with DataLoader workers set to 0\n##############################################\ntraining_args = TrainingArguments(\n    output_dir=\"./geometry_vlm_model\",\n    run_name=\"geometry_vlm_finetuning\",\n    eval_strategy=\"epoch\",\n    learning_rate=2e-4,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=4,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    save_total_limit=3,\n    fp16=True,\n    logging_steps=10,\n    save_strategy=\"epoch\",\n    dataloader_num_workers=0  # Set to 0 to disable multi-worker data loading\n)\n\n##############################################\n# Prepare training and evaluation datasets\n##############################################\ntrain_dataset = GeometryDataset(processed_dataset[\"train\"], processor)\neval_dataset = GeometryDataset(processed_dataset[\"validation\"], processor)\n\n##############################################\n# Minimal Forward-Backward Dry Run\n##############################################\nprint(\"Running a minimal forward-backward pass to validate the training loop...\")\nsample = train_dataset[0]\nfor key in sample:\n    sample[key] = sample[key].unsqueeze(0)  # add batch dimension\n\n# Move sample tensors to device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nfor key, tensor in sample.items():\n    sample[key] = tensor.to(device)\n\npeft_model.to(device)\npeft_model.train()\noptimizer = torch.optim.Adam(peft_model.parameters(), lr=training_args.learning_rate)\n\n# Forward pass\noutputs = peft_model(**sample)\nloss = outputs.loss\nprint(\"Dry run loss:\", loss.item())\n\n# Backward pass\nloss.backward()\noptimizer.step()\nprint(\"Dry run forward-backward pass completed successfully.\\n\")\n\n##############################################\n# Initialize Trainer and start training\n##############################################\ntrainer = Trainer(\n    model=peft_model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    data_collator=collate_fn,\n)\n\nprint(\"Starting training...\")\ntrainer.train()\nprint(\"Training completed.\")\n\n##############################################\n# Save the fine-tuned model and processor\n##############################################\npeft_model.save_pretrained(\"./geometry_vlm_final\")\nprocessor.save_pretrained(\"./geometry_vlm_final\")\nprint(\"Model and processor saved.\")\n\n##############################################\n# Inference and Evaluation Functions\n##############################################\ndef predict(model, processor, image, problem, choices):\n    prompt = f\"\"\"\n    Please solve this geometry problem.\n    \n    Problem: {problem}\n    \n    Options:\n    A: {choices[0]}\n    B: {choices[1]}\n    C: {choices[2]}\n    D: {choices[3]}\n    \n    The correct answer is: \n    \"\"\"\n    inputs = processor(\n        text=prompt,\n        images=image,\n        return_tensors=\"pt\"\n    )\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=5,\n            num_beams=4,\n            do_sample=False\n        )\n    prediction = processor.decode(outputs[0], skip_special_tokens=True)\n    prediction = prediction.strip()\n    if len(prediction) >= 1:\n        return prediction[0]\n    else:\n        return \"\"\n\ndef evaluate_model(model, dataset, processor):\n    model.eval()\n    predictions = []\n    ground_truths = []\n    ids = []\n    \n    for i, item in enumerate(dataset):\n        pred = predict(model, processor, item[\"image\"], item[\"problem_text\"], item[\"choices\"])\n        predictions.append(pred)\n        ground_truths.append(item[\"ground_truth\"])\n        ids.append(item[\"id\"])\n        if (i + 1) % 10 == 0:\n            print(f\"Processed {i+1}/{len(dataset)} examples\")\n    \n    accuracy = accuracy_score(ground_truths, predictions)\n    report = classification_report(ground_truths, predictions)\n    results_df = pd.DataFrame({\n        \"id\": ids,\n        \"prediction\": predictions,\n        \"ground_truth\": ground_truths,\n        \"correct\": [p == g for p, g in zip(predictions, ground_truths)]\n    })\n    \n    return {\n        \"accuracy\": accuracy,\n        \"report\": report,\n        \"results\": results_df\n    }\n\n##############################################\n# Evaluate on Test Data and Save Predictions\n##############################################\nprint(\"Evaluating on test data...\")\ntest_results = evaluate_model(peft_model, processed_dataset[\"test\"], processor)\nprint(f\"Test Accuracy: {test_results['accuracy']}\")\nprint(test_results['report'])\ntest_results['results'].to_csv(\"geometry_predictions_results.csv\", index=False)\nprint(\"Evaluation complete and results saved.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T18:41:20.258911Z","iopub.execute_input":"2025-04-08T18:41:20.259262Z","iopub.status.idle":"2025-04-08T18:44:19.284319Z","shell.execute_reply.started":"2025-04-08T18:41:20.259230Z","shell.execute_reply":"2025-04-08T18:44:19.283110Z"}},"outputs":[{"name":"stdout","text":"Train samples: 2101\nValidation samples: 300\nTest samples: 501\nFirst training sample:\n{'images': [<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=648x405 at 0x7B7137DA1330>], 'problem': '<image>Find $x$.', 'answer': '2', 'id': 84, 'choices': ['1', '2', '3', '4'], 'ground_truth': 'B'}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/505 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48f9f81020604358b6a3c7ea0bce18c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.45k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a53dd7972bf64dd298bb90600f707dbb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e09e6c41672448bd81256988931e9a31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/3.62M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f40ac8f7922464f932056aec718f235"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/41.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71ba5319025c4dfdbec992955d7e31e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/552 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d29ff3c97b5247bdbb28473b1eb003e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"processor_config.json:   0%|          | 0.00/173 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a56a60dbdff4ed3b844943ee79625c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.json:   0%|          | 0.00/701 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0310f91472f4411b9c221e22f4e83cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/950 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7ed8f28fde84221acf9b1e133ec6917"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/70.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a60e2b0fd4f43c48732a9b398340ded"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfea9d006b7b400586c08b596f8b1ba2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ccd3046e0c74bebad0211ff6f9e9205"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f64b37f84af43868f1f49f261ca2729"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.18G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad51f96310cc4d04824e8b698052d050"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e85d3093b50f440485adbd63f3a44d7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/141 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7aa3586278724077837cf5839a144282"}},"metadata":{}},{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"Trainable parameters: 42336256\nTotal parameters: 7105763328\nPercentage of trainable parameters: 0.60%\nRunning a minimal forward-backward pass to validate the training loop...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\nExpanding inputs for image tokens in LLaVa should be done in processing. Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. Using processors without these attributes in the config is deprecated and will throw an error in v4.50.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-a9993916b027>\u001b[0m in \u001b[0;36m<cell line: 274>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpeft_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dry run loss:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1717\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1719\u001b[0;31m                 return self.base_model(\n\u001b[0m\u001b[1;32m   1720\u001b[0m                     \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_injection_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llava/modeling_llava.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, pixel_values, attention_mask, position_ids, past_key_values, inputs_embeds, vision_feature_layer, vision_feature_select_strategy, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep)\u001b[0m\n\u001b[1;32m    490\u001b[0m             \u001b[0;31m# prefill stage vs decoding stage (legacy behavior copied)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m                 inputs_embeds, attention_mask, labels, position_ids = self._merge_input_ids_with_image_features(\n\u001b[0m\u001b[1;32m    493\u001b[0m                     \u001b[0mimage_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llava/modeling_llava.py\u001b[0m in \u001b[0;36m_merge_input_ids_with_image_features\u001b[0;34m(self, image_features, inputs_embeds, input_ids, attention_mask, labels)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;31m# 5. Fill the embeddings corresponding to the images. Anything that is not `text_positions` needs filling (#29835)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m         image_to_overwrite = torch.full(\n\u001b[0m\u001b[1;32m    354\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_embed_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         )\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"],"ename":"RuntimeError","evalue":"CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","output_type":"error"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}